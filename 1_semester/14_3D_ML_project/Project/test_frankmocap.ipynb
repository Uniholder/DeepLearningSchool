{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00cef878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as osp\n",
    "import torch\n",
    "from torchvision.transforms import Normalize\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from demo.demo_options import DemoOptions\n",
    "from bodymocap.body_mocap_api import BodyMocap\n",
    "from bodymocap.body_bbox_detector import BodyPoseEstimator\n",
    "import mocap_utils.demo_utils as demo_utils\n",
    "import mocap_utils.general_utils as gnu\n",
    "from mocap_utils.timer import Timer\n",
    "\n",
    "import renderer.image_utils as imu\n",
    "# from renderer.viewer2D import ImShow\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683372e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Body Pose Estimator\n"
     ]
    }
   ],
   "source": [
    "# Set bbox detector\n",
    "body_bbox_detector = BodyPoseEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b233765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "# Set mocap regressor\n",
    "checkpoint_path = './extra_data/body_module/pretrained_weights/2020_05_31-00_50_43-best-51.749683916568756.pt'\n",
    "smpl_dir = './extra_data/smpl/'\n",
    "body_mocap = BodyMocap(checkpoint_path, smpl_dir, device, use_smplx=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d796b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer_type = 'pytorch3d'\n",
    "if renderer_type in ['pytorch3d', 'opendr']:\n",
    "    from renderer.screen_free_visualizer import Visualizer\n",
    "else:\n",
    "    from renderer.visualizer import Visualizer\n",
    "visualizer = Visualizer(renderer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83835652",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_frame = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74b2f5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Bbox saved: ./mocap_output/bbox/00000_bbox.json\n",
      "Visualization saved: ./mocap_output/rendered/00000.jpg\n",
      "Prediction saved: ./mocap_output/mocap/00000_prediction_result.pkl\n",
      "Time: 0.62 sec/frame, FPS 1.61\n",
      "Processed : ./mocap_output/frames/00000.jpg\n",
      ">> Generating video in ./mocap_output/han_short.mp4\n"
     ]
    }
   ],
   "source": [
    "cur_frame = start_frame\n",
    "video_frame = 0\n",
    "timer = Timer()\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.input_path = './sample_data/han_short.mp4'\n",
    "        self.input_type = 'video'\n",
    "        self.save_frame = True\n",
    "        self.out_dir = './mocap_output'\n",
    "        self.end_frame = float('inf')\n",
    "        self.save_bbox_output = True\n",
    "        self.single_person = False\n",
    "        self.no_display = True\n",
    "        self.save_pred_pkl = True\n",
    "        self.use_smplx = False\n",
    "        self.save_mesh = True\n",
    "        self.no_video_out = False\n",
    "\n",
    "args = Args()\n",
    "        \n",
    "input_type, input_data = demo_utils.setup_input(args)\n",
    "\n",
    "while True:\n",
    "    timer.tic()\n",
    "    # load data\n",
    "\n",
    "#     if input_type =='image_dir':\n",
    "#         if cur_frame < len(input_data):\n",
    "#             image_path = input_data[cur_frame]\n",
    "#             img_original_bgr  = cv2.imread(image_path)\n",
    "#         else:\n",
    "#             img_original_bgr = None\n",
    "\n",
    "    if input_type == 'video':      \n",
    "        _, img_original_bgr = input_data.read()\n",
    "        if video_frame < cur_frame:\n",
    "            video_frame += 1\n",
    "            continue\n",
    "        # save the obtained video frames\n",
    "        image_path = osp.join(args.out_dir, \"frames\", f\"{cur_frame:05d}.jpg\")\n",
    "        if img_original_bgr is not None:\n",
    "            video_frame += 1\n",
    "            if args.save_frame:\n",
    "                gnu.make_subdir(image_path)\n",
    "                cv2.imwrite(image_path, img_original_bgr)\n",
    "    else:\n",
    "        assert False, \"Unknown input_type\"\n",
    "\n",
    "    cur_frame +=1\n",
    "    if img_original_bgr is None or cur_frame > args.end_frame:\n",
    "        break   \n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    body_pose_list, body_bbox_list = body_bbox_detector.detect_body_pose(img_original_bgr)\n",
    "    hand_bbox_list = [None, ] * len(body_bbox_list)\n",
    "\n",
    "    # save the obtained body & hand bbox to json file\n",
    "    if args.save_bbox_output: \n",
    "        demo_utils.save_info_to_json(args, image_path, body_bbox_list, hand_bbox_list)\n",
    "\n",
    "    if len(body_bbox_list) < 1: \n",
    "        print(f\"No body deteced: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    #Sort the bbox using bbox size \n",
    "    # (to make the order as consistent as possible without tracking)\n",
    "    bbox_size =  [ (x[2] * x[3]) for x in body_bbox_list]\n",
    "    idx_big2small = np.argsort(bbox_size)[::-1]\n",
    "    body_bbox_list = [ body_bbox_list[i] for i in idx_big2small ]\n",
    "    if args.single_person and len(body_bbox_list)>0:\n",
    "        body_bbox_list = [body_bbox_list[0], ]       \n",
    "\n",
    "    # Body Pose Regression\n",
    "    pred_output_list = body_mocap.regress(img_original_bgr, body_bbox_list)\n",
    "    assert len(body_bbox_list) == len(pred_output_list)\n",
    "\n",
    "    # extract mesh for rendering (vertices in image space and faces) from pred_output_list\n",
    "    pred_mesh_list = demo_utils.extract_mesh_from_output(pred_output_list)\n",
    "    \n",
    "    # visualization\n",
    "    res_img = visualizer.visualize(\n",
    "        img_original_bgr,\n",
    "        pred_mesh_list = pred_mesh_list, \n",
    "        body_bbox_list = body_bbox_list)\n",
    "\n",
    "    # show result in the screen\n",
    "#     if not args.no_display:\n",
    "#         res_img = res_img.astype(np.uint8)\n",
    "#         ImShow(res_img)\n",
    "\n",
    "    # save result image\n",
    "    if args.out_dir is not None:\n",
    "        demo_utils.save_res_img(args.out_dir, image_path, res_img)\n",
    "\n",
    "    # save predictions to pkl\n",
    "    if args.save_pred_pkl:\n",
    "        demo_type = 'body'\n",
    "        demo_utils.save_pred_to_pkl(\n",
    "            args, demo_type, image_path, body_bbox_list, hand_bbox_list, pred_output_list)\n",
    "\n",
    "    timer.toc(bPrint=True,title=\"Time\")\n",
    "    print(f\"Processed : {image_path}\")\n",
    "    break\n",
    "\n",
    "#save images as a video\n",
    "if not args.no_video_out and input_type in ['video', 'webcam']:\n",
    "    demo_utils.gen_video_out(args.out_dir, args.seq_name)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53344473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'vertices': array([[404.13864 , 128.8618  , -26.815046],\n",
       "         [402.08905 , 131.59969 , -29.17342 ],\n",
       "         [404.93375 , 133.3429  , -27.826658],\n",
       "         ...,\n",
       "         [375.07648 , 144.8882  ,  -4.135   ],\n",
       "         [375.08624 , 144.30122 ,  -4.34855 ],\n",
       "         [374.25165 , 144.62494 ,  -5.059637]], dtype=float32),\n",
       "  'faces': array([[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]], dtype=int32)}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mesh_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1aee1f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6890, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mesh_list[0]['vertices'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43c0c4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13776, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mesh_list[0]['faces'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566f35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
