{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0ee89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as osp\n",
    "import torch\n",
    "from torchvision.transforms import Normalize\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "import _pickle as pkl\n",
    "from datetime import datetime\n",
    "\n",
    "from demo.demo_options import DemoOptions\n",
    "from bodymocap.body_mocap_api import BodyMocap\n",
    "from bodymocap.body_bbox_detector import BodyPoseEstimator\n",
    "import mocap_utils.demo_utils as demo_utils\n",
    "import mocap_utils.general_utils as gnu\n",
    "from mocap_utils.timer import Timer\n",
    "\n",
    "import renderer.image_utils as imu\n",
    "from renderer.screen_free_visualizer import Visualizer\n",
    "from renderer import p3d_renderer\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8abe8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists, join, split\n",
    "from glob import glob\n",
    "from psbody.mesh import Mesh, MeshViewer, MeshViewers\n",
    "\n",
    "from MultiGarmentNetwork.utils.smpl_paths import SmplPaths\n",
    "from MultiGarmentNetwork.lib.ch_smpl import Smpl\n",
    "from MultiGarmentNetwork.dress_SMPL import load_smpl_from_file, pose_garment, dress\n",
    "from MultiGarmentNetwork.utils.interpenetration_ind import remove_interpenetration_fast\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d161b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Body Pose Estimator\n"
     ]
    }
   ],
   "source": [
    "# Set bbox detector\n",
    "body_bbox_detector = BodyPoseEstimator()\n",
    "\n",
    "use_smplx = False\n",
    "\n",
    "# Set mocap regressor\n",
    "checkpoint_path = './extra_data/body_module/pretrained_weights/2020_05_31-00_50_43-best-51.749683916568756.pt'\n",
    "# checkpoint_path = './extra_data/body_module/pretrained_weights/smplx-03-28-46060-w_spin_mlc3d_46582-2089_2020_03_28-21_56_16.pt'\n",
    "smpl_dir = './extra_data/smpl/'\n",
    "body_mocap = BodyMocap(checkpoint_path, smpl_dir, device, use_smplx=use_smplx)\n",
    "\n",
    "visualizer = Visualizer('pytorch3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9db796cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MGN\n",
    "path = '../Multi-Garment_dataset/'\n",
    "garment_classes = ['Pants', 'ShortPants', 'ShirtNoCoat', 'TShirtNoCoat', 'LongCoat']\n",
    "gar_dict = {}\n",
    "for gar in garment_classes:\n",
    "    gar_dict[gar] = glob(join(path, '*', gar + '.obj'))\n",
    "\n",
    "dp = SmplPaths()\n",
    "# vt, ft = dp.get_vt_ft_hres()\n",
    "\n",
    "## This file contains correspondances between garment vertices and smpl body\n",
    "fts_file = 'MultiGarmentNetwork/assets/garment_fts.pkl'\n",
    "vert_indices, fts = pkl.load(open(fts_file, 'rb') , encoding='latin1')\n",
    "# fts['naked'] = ft\n",
    "\n",
    "## Choose any garmet type as source\n",
    "garment_type = 'Pants'\n",
    "index = np.random.randint(0, len(gar_dict[garment_type]))   ## Randomly pick from the digital wardrobe\n",
    "path = split(gar_dict[garment_type][index])[0]\n",
    "    \n",
    "garment_org_body_unposed = load_smpl_from_file(join(path, 'registration.pkl'))\n",
    "garment_org_body_unposed.pose[:] = 0\n",
    "garment_org_body_unposed.trans[:] = 0\n",
    "garment_org_body_unposed = Mesh(garment_org_body_unposed.v, garment_org_body_unposed.f)\n",
    "\n",
    "garment_unposed = Mesh(filename=join(path, garment_type + '.obj'))\n",
    "garment_tex = join(path, 'multi_tex.jpg')\n",
    "tgt_body = Mesh(smpl.r, smpl.f)\n",
    "\n",
    "vert_inds = vert_indices[garment_type]\n",
    "garment_unposed.set_texture_image(garment_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9cca4532",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "310 453 115 473\n",
      "Using medium size renderer\n",
      "428\n",
      "Visualization saved: ./mocap_output/rendered/00000.jpg\n",
      "Time: 0.53 sec/frame, FPS 1.90\n",
      "Processed : ./mocap_output/frames/00000.jpg\n"
     ]
    }
   ],
   "source": [
    "start_frame = 0\n",
    "cur_frame = start_frame\n",
    "video_frame = 0\n",
    "timer = Timer()\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.input_path = './sample_data/han_short.mp4'\n",
    "        self.input_type = 'video'\n",
    "        self.save_frame = False\n",
    "        self.out_dir = './mocap_output'\n",
    "        self.end_frame = float('inf')\n",
    "        self.save_bbox_output = False\n",
    "        self.single_person = False\n",
    "        self.no_display = True\n",
    "        self.save_pred_pkl = True\n",
    "        self.use_smplx = True\n",
    "        self.save_mesh = True\n",
    "        self.no_video_out = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "use_smplx = args.use_smplx\n",
    "        \n",
    "input_type, input_data = demo_utils.setup_input(args)\n",
    "\n",
    "while True:\n",
    "    timer.tic()\n",
    "    # load data\n",
    "    if input_type == 'video':      \n",
    "        _, img_original_bgr = input_data.read()\n",
    "        if video_frame < cur_frame:\n",
    "            video_frame += 1\n",
    "            continue\n",
    "        # save the obtained video frames\n",
    "        image_path = osp.join(args.out_dir, \"frames\", f\"{cur_frame:05d}.jpg\")\n",
    "        if img_original_bgr is not None:\n",
    "            video_frame += 1\n",
    "            if args.save_frame:\n",
    "                gnu.make_subdir(image_path)\n",
    "                cv2.imwrite(image_path, img_original_bgr)\n",
    "    else:\n",
    "        assert False, \"Unknown input_type\"\n",
    "\n",
    "    cur_frame +=1\n",
    "    if img_original_bgr is None or cur_frame > args.end_frame:\n",
    "        break   \n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    body_pose_list, body_bbox_list = body_bbox_detector.detect_body_pose(img_original_bgr)\n",
    "\n",
    "    if len(body_bbox_list) < 1: \n",
    "        print(f\"No body detected: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    #Sort the bbox using bbox size \n",
    "    # (to make the order as consistent as possible without tracking)\n",
    "    bbox_size =  [(x[2] * x[3]) for x in body_bbox_list]\n",
    "    idx_big2small = np.argsort(bbox_size)[::-1]\n",
    "    body_bbox_list = [ body_bbox_list[i] for i in idx_big2small ]\n",
    "    if args.single_person and len(body_bbox_list) > 0:\n",
    "        body_bbox_list = [body_bbox_list[0], ]       \n",
    "\n",
    "    # Body Pose Regression\n",
    "    pred_output_list = body_mocap.regress(img_original_bgr, body_bbox_list)\n",
    "    assert len(body_bbox_list) == len(pred_output_list)\n",
    "    \n",
    "    # extract mesh for rendering (vertices in image space and faces) from pred_output_list\n",
    "#     pred_mesh_list = demo_utils.extract_mesh_from_output(pred_output_list)\n",
    "    \n",
    "#     smpl = Smpl(dp.get_hres_smpl_model_data())\n",
    "#     smpl.pose[:] = pred_output_list[0]['pred_body_pose'][0] * 0.05\n",
    "#     smpl.betas[:] = pred_output_list[0]['pred_betas'][0] * 0.01\n",
    "#     smpl.trans[:] = 0\n",
    "    \n",
    "#     new_garment = dress(smpl, garment_org_body_unposed, garment_unposed, vert_inds, garment_tex)\n",
    "    \n",
    "#     v_f = [{'vertices': new_garment.v, 'faces': new_garment.f.astype('int32')}]\n",
    "    # visualization\n",
    "    \n",
    "#     renderer = Pytorch3dRenderer(480, )\n",
    "#     renderer.render(verts=new_garment.v, faces=new_garment.f, bg_img=img_original_bgr)\n",
    "    \n",
    "    res_img = visualizer.visualize(\n",
    "        img_original_bgr,\n",
    "        pred_mesh_list = pred_mesh_list, \n",
    "        body_bbox_list = body_bbox_list)\n",
    "\n",
    "    # save result image\n",
    "    if args.out_dir is not None:\n",
    "        demo_utils.save_res_img(args.out_dir, image_path, res_img)\n",
    "\n",
    "    timer.toc(bPrint=True,title=\"Time\")\n",
    "    print(f\"Processed : {image_path}\")\n",
    "    break\n",
    "\n",
    "#save images as a video\n",
    "# if not args.no_video_out and input_type in ['video', 'webcam']:\n",
    "#     demo_utils.gen_video_out(args.out_dir, args.seq_name)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "953f6352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.91917896e+00,  4.58017327e-02,  1.53551832e-01,\n",
       "        -9.22530651e-01,  8.30429792e-02,  3.82127285e-01,\n",
       "        -8.67569566e-01, -9.41842701e-03, -3.08711380e-01,\n",
       "         5.51593065e-01, -2.62972210e-02,  1.17724165e-02,\n",
       "         1.55291820e+00,  9.38382372e-02, -2.03269675e-01,\n",
       "         1.60624826e+00, -1.72855295e-02,  1.70969218e-01,\n",
       "        -5.60724996e-02, -2.51412056e-02, -6.79262029e-03,\n",
       "        -1.70694739e-01,  7.73771927e-02, -6.54179528e-02,\n",
       "        -2.13481039e-01, -2.78314129e-02, -1.45036180e-03,\n",
       "        -3.69013101e-02, -1.13427769e-02, -8.66152346e-03,\n",
       "        -2.59527802e-01,  6.41327873e-02,  1.68739066e-01,\n",
       "        -2.42282629e-01, -1.39641911e-01, -1.29240543e-01,\n",
       "        -3.32024634e-01, -9.92928073e-02, -7.45689273e-02,\n",
       "         5.03234453e-02, -2.15491727e-01, -3.47436190e-01,\n",
       "         3.79543081e-02,  1.83946118e-01,  3.41378152e-01,\n",
       "        -4.32795621e-02, -8.62272084e-02,  1.32593354e-02,\n",
       "         9.13528800e-02, -3.66373211e-01, -8.13994765e-01,\n",
       "         8.22413862e-02,  2.69980103e-01,  8.60714853e-01,\n",
       "         2.12322161e-01, -9.02887702e-01,  2.42460236e-01,\n",
       "         1.48995027e-01,  7.73568034e-01, -1.55371800e-01,\n",
       "         2.44113714e-01, -3.91888991e-02,  2.08952576e-01,\n",
       "         1.52510270e-01,  3.13121825e-02, -1.60491675e-01,\n",
       "        -2.35091552e-01, -1.13584720e-01, -2.44322896e-01,\n",
       "        -2.06727579e-01,  1.06525600e-01,  2.20032185e-01]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_output_list[0]['pred_body_pose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec03a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl.pose[:] = np.random.randn(72) *0.05\n",
    "smpl.betas[:] = np.random.randn(10) *0.01\n",
    "smpl.trans[:] = 0\n",
    "tgt_body = Mesh(smpl.r, smpl.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "674a61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_inds = vert_indices[garment_type]\n",
    "garment_unposed.set_texture_image(garment_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "35cc1ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.073349,  0.261848,  0.031269],\n",
       "       [ 0.08271 ,  0.270592,  0.012117],\n",
       "       [ 0.078152,  0.277105,  0.0038  ],\n",
       "       ...,\n",
       "       [-0.102649,  0.121517,  0.09446 ],\n",
       "       [-0.094741,  0.115864,  0.102627],\n",
       "       [-0.099849,  0.114106,  0.098278]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garment_unposed.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e2f0470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06929031,  0.26804627,  0.01835646],\n",
       "       [ 0.07501148,  0.27842027, -0.00282235],\n",
       "       [ 0.06833248,  0.28578541, -0.01007526],\n",
       "       ...,\n",
       "       [-0.09944983,  0.11889105,  0.13553003],\n",
       "       [-0.0898118 ,  0.11403581,  0.14393182],\n",
       "       [-0.09602736,  0.11176193,  0.1400399 ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_garment.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c9a454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_garment = dress(smpl, garment_org_body_unposed, garment_unposed, vert_inds, garment_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b38867bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1920\n",
    "renderer = Pytorch3dRenderer(img_size=input_size, mesh_color=[0.8, 0.53, 0.53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2b8aa9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rend_img = np.ones((input_size, input_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40eb4fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1026 905 -588 580\n",
      "Using large size renderer\n",
      "1098\n"
     ]
    }
   ],
   "source": [
    "res_img = renderer.render(verts=new_garment.v * 1920, faces=new_garment.f.astype('int32'), bg_img=img_original_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "104a42e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved: ./mocap_output/rendered/00000.jpg\n"
     ]
    }
   ],
   "source": [
    "demo_utils.save_res_img(args.out_dir, image_path, res_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797a21f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
