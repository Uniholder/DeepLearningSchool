{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": " [homework]classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7f35a8cb249947b6b5b76e95facc6868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3bf6dc9a74924ce1849cc1a21fed37e3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_85ec73bab535416a9194dba51c6718a8",
              "IPY_MODEL_56193bcac9da4a8caef4978804c4d5b8",
              "IPY_MODEL_203a91bc60ca4b25921a1560332ed00e"
            ]
          }
        },
        "3bf6dc9a74924ce1849cc1a21fed37e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85ec73bab535416a9194dba51c6718a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_02e8354ecae748f88b3f4ec3bb4e1fac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 1:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_74e910f42e1d407fb2620ce7320c4d59"
          }
        },
        "56193bcac9da4a8caef4978804c4d5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14bf556da80a44c2a352a33a0efb87e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 274,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_64a9c203a729415888775767fa34159c"
          }
        },
        "203a91bc60ca4b25921a1560332ed00e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d7295fb3223948cf96a0966eca870b02",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/274 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a33bfb28a8a4314bd966504ea61f20c"
          }
        },
        "02e8354ecae748f88b3f4ec3bb4e1fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "74e910f42e1d407fb2620ce7320c4d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14bf556da80a44c2a352a33a0efb87e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "64a9c203a729415888775767fa34159c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7295fb3223948cf96a0966eca870b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a33bfb28a8a4314bd966504ea61f20c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1bc4c166a1f84a68990e93d440742e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_07ed78780df64d818e253a6002e12f5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e50b6a6353d84a6c910cccaaf8a8f7ba",
              "IPY_MODEL_5570f5710e304bd9b0da19e78da54503",
              "IPY_MODEL_9571924ba83741a0b976bd762d0c4dd3"
            ]
          }
        },
        "07ed78780df64d818e253a6002e12f5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e50b6a6353d84a6c910cccaaf8a8f7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ab0e2849fc1c4d14ae9f50ac02bba8ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_74563e8618ac4b90a58f8232e981b1a0"
          }
        },
        "5570f5710e304bd9b0da19e78da54503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3d42c90e327e4cfbb37e90b63ee7fce9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 391,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 391,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fbbb97f8644d4fafa733a1073be32aee"
          }
        },
        "9571924ba83741a0b976bd762d0c4dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c78ff6dc60bb45378178ee1a1721cda2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 391/391 [00:13&lt;00:00,  6.60it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a37c8e9f33e744c4a83b3f11fff3c78b"
          }
        },
        "ab0e2849fc1c4d14ae9f50ac02bba8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "74563e8618ac4b90a58f8232e981b1a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d42c90e327e4cfbb37e90b63ee7fce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fbbb97f8644d4fafa733a1073be32aee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c78ff6dc60bb45378178ee1a1721cda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a37c8e9f33e744c4a83b3f11fff3c78b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uniholder/DeepLearningSchool/blob/main/2_semester/3_RNN/%5Bhomework%5Dclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot3c4fjZwC4T"
      },
      "source": [
        "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
        "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2JdzEXmwRU5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TItJgQMp5p4E"
      },
      "source": [
        "# Задание 3\n",
        "\n",
        "## Классификация текстов\n",
        "\n",
        "В этом задании вам предстоит попробовать несколько методов, используемых в задаче классификации, а также понять насколько хорошо модель понимает смысл слов и какие слова в примере влияют на результат."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1emhHMhniZMu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torchtext.legacy import datasets\n",
        "from torchtext.legacy.data import Field, LabelField, BucketIterator, dataset\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from tqdm.autonotebook import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyOlPZA26Ppy"
      },
      "source": [
        "В этом задании мы будем использовать библиотеку torchtext. Она довольна проста в использовании и поможет нам сконцентрироваться на задаче, а не на написании Dataloader-а."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VuQ1E10_tX_"
      },
      "source": [
        "TEXT = Field(sequential=True, lower=True, include_lengths=True)  # Поле текста\n",
        "LABEL = LabelField(dtype=torch.float)  # Поле метки"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBK4ipzS122j"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNWLG7mG6n2d"
      },
      "source": [
        "Датасет на котором мы будем проводить эксперементы это комментарии к фильмам из сайта IMDB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRzUSWeAi6Xq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8196d9d-aef2-4838-ea56-e39ed2e2f9c6"
      },
      "source": [
        "train, test = datasets.IMDB.splits(TEXT, LABEL)  # загрузим датасет\n",
        "train, valid = train.split(random_state=random.seed(SEED))  # разобьем на части"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:05<00:00, 14.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQfIRhWPjURL"
      },
      "source": [
        "TEXT.build_vocab(train)\n",
        "LABEL.build_vocab(train)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSoBkdcj4roR"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size = 64,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_CRDES360wG"
      },
      "source": [
        "## RNN\n",
        "\n",
        "Для начала попробуем использовать рекурентные нейронные сети. На семинаре вы познакомились с GRU, вы можете также попробовать LSTM. Можно использовать для классификации как hidden_state, так и output последнего токена."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB0W00WP_MyV"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.w_h = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
        "        self.b_h = nn.Parameter(torch.rand((1, hidden_size)))\n",
        "        self.w_x = nn.Parameter(torch.rand(embed_size, hidden_size))\n",
        "        self.b_x = nn.Parameter(torch.rand(1, hidden_size))\n",
        "        self.w_yh = nn.Parameter(torch.rand(1, hidden_size))\n",
        "        self.b_yh = nn.Parameter(torch.rand(1, hidden_size))\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        '''\n",
        "        x – torch.FloatTensor with the shape (bs, seq_length, emb_size)\n",
        "        hidden - torch.FloatTensro with the shape (bs, hidden_size)\n",
        "        return: torch.FloatTensor with the shape (bs, hidden_size)\n",
        "        '''\n",
        "        if hidden is None:\n",
        "            hidden = torch.zeros((x.size(0), self.hidden_size)).to(x.device)\n",
        "        seq_length = x.size(1)\n",
        "        for cur_idx in range(seq_length):\n",
        "            hidden = torch.tanh(\n",
        "                x[:, cur_idx] @ self.w_x + self.b_x + hidden @ self.w_h + self.b_h\n",
        "            )\n",
        "            # y = torch.tanh(\n",
        "            #     hidden @ self.w_yh + self.b_yh\n",
        "            # )\n",
        "        return hidden"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yZVCXwIWnNq",
        "outputId": "f1cf2f09-4e58-4c94-f910-496a8714836f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.rnn.w_x.shape"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxj-N9syW0np",
        "outputId": "41f0951e-881f-4aba-8a4d-bc6d1c1ce971",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedded.shape"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([161, 64, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "746DmYK3YAeT",
        "outputId": "94fbb39d-5073-4251-b123-3b90c5a483b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hidden = torch.zeros((161, 256)).to(device)\n",
        "hidden.shape"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([161, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjeg_kpmXiYJ",
        "outputId": "fdf21671-b93d-42e7-f65d-2819e75b2895",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(embedded[:, 1] @ model.rnn.w_x).shape + hidden @ "
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([161, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1yE1KPQqDat"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(\n",
        "            self, \n",
        "            vocab_size, \n",
        "            embedding_dim, \n",
        "            hidden_dim, \n",
        "            output_dim, \n",
        "            n_layers, \n",
        "            bidirectional, \n",
        "            dropout, \n",
        "            pad_idx\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.rnn = RNN(embed_size=embedding_dim, hidden_size=hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        '''\n",
        "        text: [sent len, batch size]\n",
        "        embedded: [sent len, batch size, emb dim]\n",
        "        hidden: [num layers * num directions, batch size, hid dim]\n",
        "        cell: [num layers * num directions, batch size, hid dim]\n",
        "        output: [sent len, batch size, hid dim * num directions]\n",
        "        hidden: [batch size, hid dim * num directions]\n",
        "        '''\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        # pack sequence\n",
        "        # packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu())\n",
        "        \n",
        "        # cell arg for LSTM, remove for GRU\n",
        "        # packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        hidden = self.rnn(embedded)\n",
        "        print('hidden:', hidden.shape)\n",
        "        # unpack sequence, output over padding tokens are zero tensors\n",
        "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        \n",
        "        # hidden = None  # YOUR CODE GOES HERE\n",
        "                \n",
        "        fc = self.fc(hidden)\n",
        "        print('fc:', fc.shape)\n",
        "            \n",
        "        return torch.sigmoid(fc)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VHFHbW002sl"
      },
      "source": [
        "# class RNNBaseline(nn.Module):\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "#                  bidirectional, dropout, pad_idx):\n",
        "        \n",
        "#         super().__init__()\n",
        "        \n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "#         self.rnn = None  # YOUR CODE GOES HERE\n",
        "        \n",
        "#         self.fc = None  # YOUR CODE GOES HERE\n",
        "        \n",
        "        \n",
        "#     def forward(self, text, text_lengths):\n",
        "        \n",
        "#         #text = [sent len, batch size]\n",
        "        \n",
        "#         embedded = self.embedding(text)\n",
        "        \n",
        "#         #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "#         #pack sequence\n",
        "#         packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        \n",
        "#         # cell arg for LSTM, remove for GRU\n",
        "#         packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "#         #unpack sequence\n",
        "#         output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)  \n",
        "\n",
        "#         #output = [sent len, batch size, hid dim * num directions]\n",
        "#         #output over padding tokens are zero tensors\n",
        "        \n",
        "#         #hidden = [num layers * num directions, batch size, hid dim]\n",
        "#         #cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "#         #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "#         #and apply dropout\n",
        "        \n",
        "#         hidden = None  # YOUR CODE GOES HERE\n",
        "                \n",
        "#         #hidden = [batch size, hid dim * num directions] or [batch_size, hid dim * num directions]\n",
        "            \n",
        "#         return self.fc(hidden)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7dLGFg4M7Te"
      },
      "source": [
        "Поиграйтесь с гиперпараметрами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTTCCMrO3LcJ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def precision(tp, fp):\n",
        "    pp = tp + fp\n",
        "    if pp != 0:\n",
        "        return tp / pp\n",
        "    else:\n",
        "        return 0\n",
        "def recall(tp, fn):\n",
        "    return tp / (tp + fn)\n",
        "def f1(precision, recall):\n",
        "    if precision + recall != 0:\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggTiORJ-8t0J"
      },
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "emb_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.2\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "patience=3"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIiM_ZBt9_91"
      },
      "source": [
        "model = RNNModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=emb_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim,\n",
        "    n_layers=n_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    dropout=dropout,\n",
        "    pad_idx=PAD_IDX\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "max_epochs = 20"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jlfEAJu9akO"
      },
      "source": [
        "Обучите сетку! Используйте любые вам удобные инструменты, Catalyst, PyTorch Lightning или свои велосипеды."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87dgw6ok9hR0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474,
          "referenced_widgets": [
            "7f35a8cb249947b6b5b76e95facc6868",
            "3bf6dc9a74924ce1849cc1a21fed37e3",
            "85ec73bab535416a9194dba51c6718a8",
            "56193bcac9da4a8caef4978804c4d5b8",
            "203a91bc60ca4b25921a1560332ed00e",
            "02e8354ecae748f88b3f4ec3bb4e1fac",
            "74e910f42e1d407fb2620ce7320c4d59",
            "14bf556da80a44c2a352a33a0efb87e6",
            "64a9c203a729415888775767fa34159c",
            "d7295fb3223948cf96a0966eca870b02",
            "8a33bfb28a8a4314bd966504ea61f20c"
          ]
        },
        "outputId": "8fb94c24-f138-49c9-c1fd-362ea4e3064d"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "min_loss = np.inf\n",
        "cur_patience = 0\n",
        "THRSH = 0.5\n",
        "\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n",
        "    pbar.set_description(f\"Epoch {epoch}\")\n",
        "    for it, batch in pbar:\n",
        "        (texts, text_lengths), labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(texts, text_lengths).squeeze()\n",
        "        loss = loss_func(prediction, labels)\n",
        "        loss.backward()\n",
        "        train_loss += loss\n",
        "        optimizer.step()\n",
        "        # break\n",
        "    train_loss /= len(train_iter)\n",
        "    val_loss = 0.0\n",
        "    fp, fn, tp = 0, 0, 0\n",
        "    model.eval()\n",
        "    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n",
        "    pbar.set_description(f\"Epoch {epoch}\")\n",
        "    for it, batch in pbar:\n",
        "        (texts, text_lengths), labels = batch\n",
        "        prediction = model(texts.T, text_lengths).squeeze()\n",
        "        val_loss += loss_func(prediction, labels)\n",
        "        conf_matrix = confusion_matrix(labels.cpu(), prediction.detach().cpu() > THRSH)\n",
        "        if len(conf_matrix) != 1:\n",
        "            _, fp_batch, fn_batch, tp_batch = conf_matrix.ravel()\n",
        "        else:\n",
        "            fp_batch, fn_batch, tp_batch = 0, 0, conf_matrix.item()\n",
        "        fp += fp_batch\n",
        "        fn += fn_batch\n",
        "        tp += tp_batch\n",
        "    val_loss /= len(valid_iter)\n",
        "    val_f1 = f1(precision(tp, fp), recall(tp, fn))\n",
        "    if val_loss < min_loss:\n",
        "        min_loss = val_loss\n",
        "        best_model = model.state_dict()\n",
        "    else:\n",
        "        cur_patience += 1\n",
        "        if cur_patience == patience:\n",
        "            cur_patience = 0\n",
        "            break\n",
        "    print(f'Epoch: {epoch}, Training Loss: {train_loss}, Validation Loss: {val_loss}, Validation F1: {val_f1}')\n",
        "    break\n",
        "\n",
        "# model.load_state_dict(best_model)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f35a8cb249947b6b5b76e95facc6868",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/274 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: torch.Size([112, 64])\n",
            "embedded: torch.Size([112, 64, 100])\n",
            "hidden: torch.Size([112, 256])\n",
            "fc: torch.Size([112, 1])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-214-74ffd453836c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    714\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([64])) must be the same as input size (torch.Size([112]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zovi9qopQitO",
        "outputId": "db3aa000-bd3c-4d55-d706-1c4b841e1adc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prediction.shape"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([266])"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK76h3pkOS73",
        "outputId": "6c561775-70ec-4875-ad19-d8bd16e61e01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_IDX).to(device)\n",
        "embed"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(201055, 100, padding_idx=1)"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHEgQpq6PLhm",
        "outputId": "d2ea2ed3-5e41-4b5a-e158-44cf45eeea01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "texts.shape"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([266, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4tkPl6QOKoy"
      },
      "source": [
        "embedded = embed(texts)\n",
        "embedded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1C9wu1DOHJH",
        "outputId": "f16b6ea1-2cb5-4768-a83b-f0e50d8746b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu())"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[-0.6130,  0.8094, -2.1992,  ..., -0.4629, -0.0504,  0.2224],\n",
              "        [-1.5500,  1.8956,  1.7375,  ...,  0.8482,  0.4123, -0.5960],\n",
              "        [ 0.0192,  1.1670,  1.2084,  ..., -0.1739, -0.5530,  1.5410],\n",
              "        ...,\n",
              "        [-0.8233,  1.2384,  0.3180,  ...,  1.6911,  1.2238,  0.2138],\n",
              "        [ 0.2398, -0.3694, -1.1212,  ..., -0.4210, -0.1493,  0.2792],\n",
              "        [-0.8958,  0.3623,  0.8426,  ...,  0.6610,  0.9344,  0.0321]],\n",
              "       device='cuda:0', grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
              "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
              "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
              "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
              "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
              "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
              "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
              "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
              "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 53, 24]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMjf-UMeNyNd",
        "outputId": "de0501f4-da0b-4541-951a-e9cffc603594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "texts.T.cpu().shape"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 161])"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5c1VN1ZMoQs",
        "outputId": "a452f9f8-c398-4c80-98df-c4d730833a4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(text_lengths.cpu())"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NDW-rtTirhM",
        "outputId": "4844c546-6745-49fb-ea5b-f7af692ad0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "1bc4c166a1f84a68990e93d440742e9c",
            "07ed78780df64d818e253a6002e12f5d",
            "e50b6a6353d84a6c910cccaaf8a8f7ba",
            "5570f5710e304bd9b0da19e78da54503",
            "9571924ba83741a0b976bd762d0c4dd3",
            "ab0e2849fc1c4d14ae9f50ac02bba8ee",
            "74563e8618ac4b90a58f8232e981b1a0",
            "3d42c90e327e4cfbb37e90b63ee7fce9",
            "fbbb97f8644d4fafa733a1073be32aee",
            "c78ff6dc60bb45378178ee1a1721cda2",
            "a37c8e9f33e744c4a83b3f11fff3c78b"
          ]
        }
      },
      "source": [
        "test_loss = 0\n",
        "fp, fn, tp = 0, 0, 0\n",
        "\n",
        "pbar = tqdm(enumerate(test_iter), total=len(test_iter), leave=False)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for it, batch in pbar:\n",
        "        (texts, text_lengths), labels = batch\n",
        "        prediction = model(texts.T, text_lengths).squeeze()\n",
        "        test_loss += loss_func(prediction, labels)\n",
        "        conf_matrix = confusion_matrix(labels.cpu(), prediction.detach().cpu() > THRSH)\n",
        "        if len(conf_matrix) > 1:\n",
        "            _, fp_batch, fn_batch, tp_batch = conf_matrix.ravel()\n",
        "        else:\n",
        "            fp_batch, fn_batch, tp_batch = 0, 0, conf_matrix.item()\n",
        "        fp += fp_batch\n",
        "        fn += fn_batch\n",
        "        tp += tp_batch\n",
        "test_loss /= len(test_iter)\n",
        "test_f1 = f1(precision(tp, fp), recall(tp, fn))\n",
        "print(f'Test Loss: {test_loss}, Validation F1: {test_f1}')"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bc4c166a1f84a68990e93d440742e9c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/391 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7083289623260498, Validation F1: 0.07571724341910677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4i-Go_ICT_U"
      },
      "source": [
        "Посчитайте f1-score вашего классификатора на тестовом датасете.\n",
        "\n",
        "**Ответ**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "744nbgvaEoyY"
      },
      "source": [
        "Эпоха|Модель|Training Loss|Validation Loss|Validation F1|Test Loss|Validation F1\n",
        "-|-|-|-|-|-|-\n",
        "1|RNN|0.7228825688362122|0.7190571427345276|0.4245776648228435|0.7219743728637695|0.41036498366088275\n",
        "2+|0.7171449661254883|0.7095386981964111|0.7095386981964111|0|0.712580144405365|0.07571724341910677"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWkbCpNECflR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzDqc3__JIMe"
      },
      "source": [
        "## CNN\n",
        "\n",
        "![](https://www.researchgate.net/publication/333752473/figure/fig1/AS:769346934673412@1560438011375/Standard-CNN-on-text-classification.png)\n",
        "\n",
        "Для классификации текстов также часто используют сверточные нейронные сети. Идея в том, что как правило сентимент содержат словосочетания из двух-трех слов, например \"очень хороший фильм\" или \"невероятная скука\". Проходясь сверткой по этим словам мы получим какой-то большой скор и выхватим его с помощью MaxPool. Далее идет обычная полносвязная сетка. Важный момент: свертки применяются не последовательно, а параллельно. Давайте попробуем!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU-76tNI-STt"
      },
      "source": [
        "TEXT = Field(sequential=True, lower=True, batch_first=True)  # batch_first тк мы используем conv  \n",
        "LABEL = LabelField(batch_first=True, dtype=torch.float)\n",
        "\n",
        "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
        "trn, vld = train.split(random_state=random.seed(SEED))\n",
        "\n",
        "TEXT.build_vocab(trn)\n",
        "LABEL.build_vocab(trn)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQpS9KKUJQVH"
      },
      "source": [
        "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "        (trn, vld, tst),\n",
        "        batch_sizes=(128, 256, 256),\n",
        "        sort=False,\n",
        "        sort_key= lambda x: len(x.src),\n",
        "        sort_within_batch=False,\n",
        "        device=device,\n",
        "        repeat=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asgbwMePPNNl"
      },
      "source": [
        "Вы можете использовать Conv2d с `in_channels=1, kernel_size=(kernel_sizes[0], emb_dim))` или Conv1d c `in_channels=emb_dim, kernel_size=kernel_size[0]`. Но хорошенько подумайте над shape в обоих случаях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPP_-0E-JYTQ"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        emb_dim,\n",
        "        out_channels,\n",
        "        kernel_sizes,\n",
        "        dropout=0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.conv_0 = None  # YOUR CODE GOES HERE\n",
        "        \n",
        "        self.conv_1 = None  # YOUR CODE GOES HERE\n",
        "        \n",
        "        self.conv_2 = None  # YOUR CODE GOES HERE\n",
        "        \n",
        "        self.fc = nn.Linear(len(kernel_sizes) * out_channels, 1)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        embedded = embedded  # may be reshape here\n",
        "        \n",
        "        conved_0 = F.relu(self.conv_0(embedded))  # may be reshape here\n",
        "        conved_1 = F.relu(self.conv_1(embedded))  # may be reshape here\n",
        "        conved_2 = F.relu(self.conv_2(embedded))  # may be reshape here\n",
        "        \n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-U_2T5oKNed"
      },
      "source": [
        "kernel_sizes = [3, 4, 5]\n",
        "vocab_size = len(TEXT.vocab)\n",
        "out_channels=64\n",
        "dropout = 0.5\n",
        "dim = 300\n",
        "\n",
        "model = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=out_channels,\n",
        "            kernel_sizes=kernel_sizes, dropout=dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC2ThnfNKPIR"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mExblVtPKRw4"
      },
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVwSgwkEKTw5"
      },
      "source": [
        "max_epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQgLCELDoOA"
      },
      "source": [
        "Обучите!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQZbJ791KXHb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "min_loss = np.inf\n",
        "\n",
        "cur_patience = 0\n",
        "\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n",
        "    pbar.set_description(f\"Epoch {epoch}\")\n",
        "    for it, batch in pbar: \n",
        "        #YOUR CODE GOES HERE\n",
        "\n",
        "    train_loss /= len(train_iter)\n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n",
        "    pbar.set_description(f\"Epoch {epoch}\")\n",
        "    for it, batch in pbar:\n",
        "        # YOUR CODE GOES HERE\n",
        "    val_loss /= len(valid_iter)\n",
        "    if val_loss < min_loss:\n",
        "        min_loss = val_loss\n",
        "        best_model = model.state_dict()\n",
        "    else:\n",
        "        cur_patience += 1\n",
        "        if cur_patience == patience:\n",
        "            cur_patience = 0\n",
        "            break\n",
        "    \n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\n",
        "model.load_state_dict(best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UVCacK0EhPR"
      },
      "source": [
        "Посчитайте f1-score вашего классификатора.\n",
        "\n",
        "**Ответ**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VspGMN0ESiS"
      },
      "source": [
        "## Интерпретируемость\n",
        "\n",
        "Посмотрим, куда смотрит наша модель. Достаточно запустить код ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye2SvjXrPgJh"
      },
      "source": [
        "!pip install -q captum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e5XPKSZO6DY"
      },
      "source": [
        "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
        "\n",
        "PAD_IND = TEXT.vocab.stoi['pad']\n",
        "\n",
        "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\n",
        "lig = LayerIntegratedGradients(model, model.embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvqWhd-fPe9e"
      },
      "source": [
        "def forward_with_softmax(inp):\n",
        "    logits = model(inp)\n",
        "    return torch.softmax(logits, 0)[0][1]\n",
        "\n",
        "def forward_with_sigmoid(input):\n",
        "    return torch.sigmoid(model(input))\n",
        "\n",
        "\n",
        "# accumalate couple samples in this array for visualization purposes\n",
        "vis_data_records_ig = []\n",
        "\n",
        "def interpret_sentence(model, sentence, min_len = 7, label = 0):\n",
        "    model.eval()\n",
        "    text = [tok for tok in TEXT.tokenize(sentence)]\n",
        "    if len(text) < min_len:\n",
        "        text += ['pad'] * (min_len - len(text))\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in text]\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    input_indices = torch.tensor(indexed, device=device)\n",
        "    input_indices = input_indices.unsqueeze(0)\n",
        "    \n",
        "    # input_indices dim: [sequence_length]\n",
        "    seq_length = min_len\n",
        "\n",
        "    # predict\n",
        "    pred = forward_with_sigmoid(input_indices).item()\n",
        "    pred_ind = round(pred)\n",
        "\n",
        "    # generate reference indices for each sample\n",
        "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
        "\n",
        "    # compute attributions and approximation delta using layer integrated gradients\n",
        "    attributions_ig, delta = lig.attribute(input_indices, reference_indices, \\\n",
        "                                           n_steps=5000, return_convergence_delta=True)\n",
        "\n",
        "    print('pred: ', LABEL.vocab.itos[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n",
        "\n",
        "    add_attributions_to_visualizer(attributions_ig, text, pred, pred_ind, label, delta, vis_data_records_ig)\n",
        "    \n",
        "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n",
        "    attributions = attributions.sum(dim=2).squeeze(0)\n",
        "    attributions = attributions / torch.norm(attributions)\n",
        "    attributions = attributions.cpu().detach().numpy()\n",
        "\n",
        "    # storing couple samples in an array for visualization purposes\n",
        "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
        "                            attributions,\n",
        "                            pred,\n",
        "                            LABEL.vocab.itos[pred_ind],\n",
        "                            LABEL.vocab.itos[label],\n",
        "                            LABEL.vocab.itos[1],\n",
        "                            attributions.sum(),       \n",
        "                            text,\n",
        "                            delta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtYy633vS8Me"
      },
      "source": [
        "interpret_sentence(model, 'It was a fantastic performance !', label=1)\n",
        "interpret_sentence(model, 'Best film ever', label=1)\n",
        "interpret_sentence(model, 'Such a great show!', label=1)\n",
        "interpret_sentence(model, 'It was a horrible movie', label=0)\n",
        "interpret_sentence(model, 'I\\'ve never watched something as bad', label=0)\n",
        "interpret_sentence(model, 'It is a disgusting movie!', label=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqIRSCWlRTOe"
      },
      "source": [
        "Попробуйте добавить свои примеры!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4URAkcWXTGBi"
      },
      "source": [
        "print('Visualize attributions based on Integrated Gradients')\n",
        "visualization.visualize_text(vis_data_records_ig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvEHEaurElu8"
      },
      "source": [
        "## Эмбеддинги слов\n",
        "\n",
        "Вы ведь не забыли, как мы можем применить знания о word2vec и GloVe. Давайте попробуем!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW46gGLNuo0q"
      },
      "source": [
        "TEXT.build_vocab(trn, vectors=)# YOUR CODE GOES HERE\n",
        "# подсказка: один из импортов пока не использовался, быть может он нужен в строке выше :)\n",
        "LABEL.build_vocab(trn)\n",
        "\n",
        "word_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "kernel_sizes = [3, 4, 5]\n",
        "vocab_size = len(TEXT.vocab)\n",
        "dropout = 0.5\n",
        "dim = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ4YwLlcltm3"
      },
      "source": [
        "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
        "trn, vld = train.split(random_state=random.seed(SEED))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        return self.fc(hidden)\n",
        "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "        (trn, vld, tst),\n",
        "        batch_sizes=(128, 256, 256),\n",
        "        sort=False,\n",
        "        sort_key= lambda x: len(x.src),\n",
        "        sort_within_batch=False,\n",
        "        device=device,\n",
        "        repeat=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5pDvZgl7Fp"
      },
      "source": [
        "model = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=64,\n",
        "            kernel_sizes=kernel_sizes, dropout=dropout)\n",
        "\n",
        "word_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "prev_shape = model.embedding.weight.shape\n",
        "\n",
        "model.embedding.weight = # инициализируйте эмбэдинги\n",
        "\n",
        "assert prev_shape == model.embedding.weight.shape\n",
        "model.to(device)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwiQjcuiFGTC"
      },
      "source": [
        "Вы знаете, что делать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNqcFHT8cT0b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "min_loss = np.inf\n",
        "\n",
        "cur_patience = 0\n",
        "\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n",
        "    pbar.set_description(f\"Epoch {epoch}\")\n",
        "    for it, batch in pbar: \n",
        "        #YOUR CODE GOES HERE\n",
        "\n",
        "    train_loss /= len(train_iter)\n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n",
        "    pbar.set_description(f\"Epoch {epoch}\")\n",
        "    for it, batch in pbar:\n",
        "        # YOUR CODE GOES HERE\n",
        "    val_loss /= len(valid_iter)\n",
        "    if val_loss < min_loss:\n",
        "        min_loss = val_loss\n",
        "        best_model = model.state_dict()\n",
        "    else:\n",
        "        cur_patience += 1\n",
        "        if cur_patience == patience:\n",
        "            cur_patience = 0\n",
        "            break\n",
        "    \n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\n",
        "model.load_state_dict(best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2IdEWJQKESg"
      },
      "source": [
        "Посчитайте f1-score вашего классификатора.\n",
        "\n",
        "**Ответ**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kizk028eRF0R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sl7h_wIRGPD"
      },
      "source": [
        "Проверим насколько все хорошо!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPCm55FLir3e"
      },
      "source": [
        "PAD_IND = TEXT.vocab.stoi['pad']\n",
        "\n",
        "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\n",
        "lig = LayerIntegratedGradients(model, model.embedding)\n",
        "vis_data_records_ig = []\n",
        "\n",
        "interpret_sentence(model, 'It was a fantastic performance !', label=1)\n",
        "interpret_sentence(model, 'Best film ever', label=1)\n",
        "interpret_sentence(model, 'Such a great show!', label=1)\n",
        "interpret_sentence(model, 'It was a horrible movie', label=0)\n",
        "interpret_sentence(model, 'I\\'ve never watched something as bad', label=0)\n",
        "interpret_sentence(model, 'It is a disgusting movie!', label=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMDazB3AlFWA"
      },
      "source": [
        "print('Visualize attributions based on Integrated Gradients')\n",
        "visualization.visualize_text(vis_data_records_ig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flCZHdAVlL7W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}